{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7eb63b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyenphungbaohuy/Desktop/python/Research-ChatBot/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2632f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f82b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyenphungbaohuy/Desktop/python/Research-ChatBot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a72c7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_PAPERS_URL = \"https://huggingface.co\"\n",
    "PDF_URL = \"https://arxiv.org/pdf/\"\n",
    "DATE = \"2025-04-10\"\n",
    "PAPER_DATE = f\"/papers/date/{DATE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7164fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import logging\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b76b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str, date: str) -> str:\n",
    "    \"\"\"Fetch HTML content from a URL with error handling.\"\"\"\n",
    "    full_url = f\"{url}{date}\"\n",
    "    try:\n",
    "        response = requests.get(full_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Failed to fetch {full_url}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link(url: str) -> dict:\n",
    "    \"\"\"Extract relevant links from a paper page with robust parsing.\"\"\"\n",
    "    links = {\n",
    "        \"arxiv_id\": \"\",\n",
    "        \"arxiv_page\": \"\",\n",
    "        \"arxiv_pdf\": \"\",\n",
    "        \"project_page\": \"\",\n",
    "        \"github_page\": \"\",\n",
    "        \"summary\": \"\",\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.warning(f\"Could not fetch {url}: {e}\")\n",
    "        return links\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find arXiv links\n",
    "    arxiv_links = soup.find_all(\"a\", href=re.compile(r'arxiv\\.org/abs/\\d+\\.\\d+'))\n",
    "    if arxiv_links:\n",
    "        href = arxiv_links[0]['href']\n",
    "        links[\"arxiv_page\"] = urljoin(url, href)\n",
    "        links[\"arxiv_id\"] = href.split(\"/\")[-1]\n",
    "        links[\"arxiv_pdf\"] = f\"{PDF_URL}{links['arxiv_id']}.pdf\"\n",
    "    \n",
    "    # Find GitHub links\n",
    "    github_links = soup.find_all(\"a\", href=re.compile(r'github\\.com'))\n",
    "    if github_links:\n",
    "        links[\"github_page\"] = urljoin(url, github_links[0]['href'])\n",
    "    \n",
    "    # Find project page\n",
    "    project_links = soup.find_all(\"a\", string=re.compile(r'project page', re.I))\n",
    "    if project_links:\n",
    "        links[\"project_page\"] = urljoin(url, project_links[0]['href'])\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f858de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_papers(base_url: str, html: str) -> list[dict]:\n",
    "    \"\"\"Parse paper entries from HTML with improved selectors.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    papers = []\n",
    "    \n",
    "    entries = soup.find_all(\"article\", class_=re.compile(r'flex flex-col overflow-hidden'))\n",
    "    if not entries:\n",
    "        logging.warning(\"No papers found - check HTML structure\")\n",
    "        return papers\n",
    "\n",
    "    for entry in entries:\n",
    "        try:\n",
    "            info = entry.find(\"a\", class_=re.compile(r'line-clamp-3'))\n",
    "            if not info:\n",
    "                continue\n",
    "\n",
    "            paper_href = info.get(\"href\")\n",
    "            title = info.get_text(strip=True)\n",
    "            paper_url = urljoin(base_url, paper_href)\n",
    "            \n",
    "            additional_links = extract_link(paper_url)\n",
    "            \n",
    "            papers.append({\n",
    "                \"title\": title,\n",
    "                \"huggingface_url\": paper_url,\n",
    "                **additional_links\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error parsing paper entry: {e}\")\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_today_papers(base_url: str, date: str) -> list[dict]:\n",
    "    \"\"\"Retrieve papers for a given date with error handling.\"\"\"\n",
    "    try:\n",
    "        html = fetch_page(base_url, date)\n",
    "        return parse_papers(base_url, html)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to get papers: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url: str) -> str:\n",
    "    \"\"\"Extract text from PDF with improved error handling.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.warning(f\"Could not download PDF {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        pdf_bytes = io.BytesIO(response.content)\n",
    "        reader = PdfReader(pdf_bytes)\n",
    "        return \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"PDF processing failed: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99345d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, summarizer, tokenizer) -> str:\n",
    "    \"\"\"Generate summary using token-aware chunking.\"\"\"\n",
    "    if not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize and chunk properly\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    chunks = [tokenizer.decode(inputs['input_ids'][0][i:i+1024]) \n",
    "              for i in range(0, len(inputs['input_ids'][0]), 512)]  # 512 overlap\n",
    "    \n",
    "    summaries = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            summary = summarizer(chunk, max_length=150, min_length=30, do_sample=False)\n",
    "            summaries.append(summary[0]['summary_text'])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Summarization failed: {e}\")\n",
    "    \n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82662d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize models\n",
    "    logging.info(\"Initializing models...\")\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    \n",
    "    # Fetch papers\n",
    "    logging.info(f\"Fetching papers for {DATE}\")\n",
    "    today_papers = get_today_papers(HUGGINGFACE_PAPERS_URL, PAPER_DATE)\n",
    "    \n",
    "    # Process each paper\n",
    "    for paper in today_papers:\n",
    "        logging.info(f\"Processing: {paper['title']}\")\n",
    "        \n",
    "        if not paper.get(\"arxiv_pdf\"):\n",
    "            logging.warning(\"No arXiv PDF available, skipping summary\")\n",
    "            continue\n",
    "            \n",
    "        text = extract_text_from_url(paper['arxiv_pdf'])\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        paper[\"summary\"] = summarize_text(text, summarizer, tokenizer)\n",
    "    \n",
    "    # Save results\n",
    "    with open(\"summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(today_papers, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    logging.info(f\"Processed {len(today_papers)} papers\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aed4ad55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e01cc1232e5427fb298e313b850fb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f311053e1a441698eb62ac75e0886f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44f86223c2840ed965d90be06a9e201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfa3dcbc2f94501a7351765933de044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e35a9e91ff4eaba1e6fb37f6e759ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b0beb68a4e42bfbca1ca5bb44be0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a88260",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chunk_length=1000\n",
    "today_papers = get_today_papers(HUGGINGFACE_PAPERS_URL, PAPER_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33ef27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 305. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=152)\n",
      "Your max_length is set to 500, but your input_length is only 246. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=123)\n",
      "Your max_length is set to 500, but your input_length is only 239. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=119)\n",
      "Your max_length is set to 500, but your input_length is only 237. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=118)\n",
      "Your max_length is set to 500, but your input_length is only 216. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=108)\n",
      "Your max_length is set to 500, but your input_length is only 254. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=127)\n",
      "Your max_length is set to 500, but your input_length is only 252. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=126)\n",
      "Your max_length is set to 500, but your input_length is only 231. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=115)\n",
      "Your max_length is set to 500, but your input_length is only 314. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=157)\n",
      "Your max_length is set to 500, but your input_length is only 287. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=143)\n",
      "Your max_length is set to 500, but your input_length is only 246. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=123)\n",
      "Your max_length is set to 500, but your input_length is only 265. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=132)\n",
      "Your max_length is set to 500, but your input_length is only 225. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=112)\n",
      "Your max_length is set to 500, but your input_length is only 243. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=121)\n",
      "Your max_length is set to 500, but your input_length is only 286. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=143)\n",
      "Your max_length is set to 500, but your input_length is only 238. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=119)\n",
      "Your max_length is set to 500, but your input_length is only 316. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=158)\n",
      "Your max_length is set to 500, but your input_length is only 273. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=136)\n",
      "Your max_length is set to 500, but your input_length is only 330. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=165)\n",
      "Your max_length is set to 500, but your input_length is only 247. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=123)\n",
      "Your max_length is set to 500, but your input_length is only 265. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=132)\n",
      "Your max_length is set to 500, but your input_length is only 325. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=162)\n",
      "Your max_length is set to 500, but your input_length is only 318. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=159)\n",
      "Your max_length is set to 500, but your input_length is only 411. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=205)\n",
      "Your max_length is set to 500, but your input_length is only 250. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=125)\n",
      "Your max_length is set to 500, but your input_length is only 409. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=204)\n",
      "Your max_length is set to 500, but your input_length is only 326. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=163)\n",
      "Your max_length is set to 500, but your input_length is only 379. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=189)\n",
      "Your max_length is set to 500, but your input_length is only 230. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=115)\n",
      "Your max_length is set to 500, but your input_length is only 288. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=144)\n",
      "Your max_length is set to 500, but your input_length is only 191. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n",
      "Your max_length is set to 500, but your input_length is only 322. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=161)\n",
      "Your max_length is set to 500, but your input_length is only 344. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=172)\n",
      "Your max_length is set to 500, but your input_length is only 332. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=166)\n",
      "Your max_length is set to 500, but your input_length is only 319. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=159)\n",
      "Your max_length is set to 500, but your input_length is only 332. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=166)\n",
      "Your max_length is set to 500, but your input_length is only 339. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=169)\n",
      "Your max_length is set to 500, but your input_length is only 346. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=173)\n"
     ]
    }
   ],
   "source": [
    "for paper in today_papers:\n",
    "    text = extract_text_from_url(paper['arxiv_pdf'])\n",
    "    chunks = [text[i:i + max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        max_length = int(len(chunk) * 0.5)\n",
    "        summarized_chunk = summarizer(chunk, max_length=max_length, min_length=50, do_sample=False)\n",
    "        summary += summarized_chunk[0]['summary_text'] + \" \"\n",
    "    paper[\"smmary\"] = summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca92705",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(today_papers, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f395353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url: str, arxiv_id: str) -> str:\n",
    "    file_dir = \"data\"\n",
    "    os.makedirs(file_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(file_dir, f\"{arxiv_id}.pdf\")\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded and saved in: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ffcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved as data/OmniSVG: A Unified Scalable Vector Graphics Generation Model.pdf\n",
      "Downloaded and saved as data/Hogwild! Inference: Parallel LLM Generation via Concurrent Attention.pdf\n"
     ]
    }
   ],
   "source": [
    "for paper in today_papers[0:1]:\n",
    "    download_pdf(paper['arxiv_pdf'], paper['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ada4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "HUGGINGFACE_API_KEY = os.enviro.get('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8359587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data From the PDF File\n",
    "def load_pdf_file(data: str) -> str:\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a79fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text_split(extracted_data=extracted_data)\n",
    "print(f\"Lenght of chunck: {len(text_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e72ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings(model_name: str):\n",
    "    embeddings =  HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f63e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m qa_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepset/roberta-base-squad2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "embedding = download_hugging_face_embeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"researchbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca79742",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(name=index_name,\n",
    "                dimension=384,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\",\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db372515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunk,\n",
    "    index_name=index_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318581ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the emebeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a84af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cceb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(text: str) -> str:\n",
    "    max_chunk_length = 1024\n",
    "    \n",
    "    chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        summarized_chunk = summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "        summary += summarized_chunk + \" \"\n",
    "    return summary.strip()\n",
    "\n",
    "def chatbot_interface(context: str):\n",
    "    print(\"Chatbot ready. Type your questions about the paper. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        question = input(\"Your question: \")\n",
    "        if question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Use the QA pipeline with the given context.\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        answer = result.get(\"answer\", \"I'm not sure about that.\")\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example paper input; you can later extend this\n",
    "    # to process multiple inputs.\n",
    "    paper_data = {\n",
    "        \"title\": (\"CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs \"\n",
    "                  \"with Controllable Puzzle Generation\"),\n",
    "        \"arxiv_pdf\": \"https://arxiv.org/pdf/2504.00043.pdf\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading PDF for paper: {paper_data['title']}\")\n",
    "        pdf_path = download_pdf(paper_data[\"arxiv_pdf\"])\n",
    "        \n",
    "        print(\"Extracting text from the PDF...\")\n",
    "        pdf_text = extract_pdf_text(pdf_path)\n",
    "        \n",
    "        print(\"Summarizing the paper...\")\n",
    "        summary = summarize_paper(pdf_text)\n",
    "        print(\"Paper Summary:\\n\", summary)\n",
    "        \n",
    "        # Start chatbot interface based on the summary.\n",
    "        chatbot_interface(summary)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"deepseek-r1\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a012b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "System Prompt for AI Research Assistant\n",
    "\n",
    "Role and Scope:\n",
    "You are a research assistant specialized in AI and large language models (LLMs). Your primary duties include discovering relevant papers on the Hugging Face platform, extracting PDF links, downloading the research documents, summarizing their contents, and indexing the results for efficient retrieval using a vectorstore database. Your output must be clear, detailed, and maintain academic rigor.\n",
    "\n",
    "Functional Responsibilities:\n",
    "\n",
    "1. Paper Discovery & Download:\n",
    "   - Crawling: Continually monitor the Hugging Face page (or pages) dedicated to AI and LLM research for new papers.\n",
    "   - Extraction: Identify and extract the PDF links from the page.\n",
    "   - Downloading: Automatically download each PDF to the local system for further processing.\n",
    "\n",
    "2. Content Summarization:\n",
    "   - Parsing: Process the downloaded PDF and extract key sections—such as the abstract, introduction, methodology, experiments, results, and conclusion.\n",
    "   - Summarizing: Generate a concise yet comprehensive summary that captures the paper’s main contributions, methods, findings, and any notable insights. Ensure the summary preserves the technical accuracy and context of the original document.\n",
    "   - Validation: Verify that every summary is faithful to the paper's content, and highlight any potential uncertainties or limitations in the document.\n",
    "\n",
    "3. Indexing with Vectorstore:\n",
    "   - Embedding: Convert the text summary or key points into vector embeddings that represent the paper’s content.\n",
    "   - Database Storage: Index these embeddings in the vectorstore to allow efficient and relevant retrieval during query time.\n",
    "   - Query Matching: When a user query is received, search the vectorstore to retrieve the most semantically related papers and provide contextualized, aggregated insights.\n",
    "\n",
    "4. User Query Response:\n",
    "   - Contextual Answers: When asked about specific topics or research areas, integrate information from the vectorstore and provide detailed answers.\n",
    "   - Direct References: When possible, refer to the paper sections (e.g., “based on the methodology described in section 3,”) to support your response.\n",
    "   - Clarity & Detail: Ensure all outputs are clear, logically structured, and include sufficient technical details, especially when addressing experimental setups or complex methodologies.\n",
    "\n",
    "Behavioral Guidelines:\n",
    "- Professional Tone: Maintain a scholarly and objective tone throughout all interactions.\n",
    "- Accuracy and Rigor: Base responses strictly on the data derived from the PDF documents. Avoid injecting personal opinions or unverified information.\n",
    "- Error Handling: If any issue arises (e.g., PDF parsing errors, incomplete downloads), log a clear error message that describes the problem and, if possible, suggest remedial steps.\n",
    "- Modularity: The agent should work in a modular fashion; for instance, if the PDF extraction fails, it must notify the user or log the error without disrupting other functionalities.\n",
    "\n",
    "Example Instruction for a Query:\n",
    "“When a user inquires about recent advances in transformer-based LLMs, search your indexed vectorstore for the most relevant papers, or the summarization of the papers. Summarize the key aspects of the top results—such as novel architectures, dataset insights, and experimental results—and provide a clear, consolidated answer that synthesizes these insights.”\n",
    "\n",
    "Usage Note:\n",
    "Embed this system prompt at the beginning of your agent’s session or configuration file to ensure that every module (crawling, summarizing, embedding, and querying) follows these guidelines. This will help the agent act in a coordinated manner, reliably reflecting the state-of-the-art research in AI LLMs from the Hugging Face page.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca170ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-based QA chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt=prompt)\n",
    "# Assume '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n",
    "\n",
    "async def main():\n",
    "    browser_config = BrowserConfig()  # Default browser configuration\n",
    "    run_config = CrawlerRunConfig(fit_markdown=True)   # Default crawl run configuration\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_config) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://huggingface.co/papers\",\n",
    "            config=Craw\n",
    "        )\n",
    "        print(result.markdown)  # Print clean markdown content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870c38f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawler\u001b[38;5;241m.\u001b[39marun(\n\u001b[1;32m      2\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://example.com\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     config\u001b[38;5;241m=\u001b[39mCrawlerRunConfig(fit_markdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Different content formats\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mhtml)         \u001b[38;5;66;03m# Raw HTML\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crawler' is not defined"
     ]
    }
   ],
   "source": [
    "result = await crawler.arun(\n",
    "    url=\"https://example.com\",\n",
    "    config=CrawlerRunConfig(fit_markdown=True)\n",
    ")\n",
    "\n",
    "# Different content formats\n",
    "print(result.html)         # Raw HTML\n",
    "print(result.cleaned_html) # Cleaned HTML\n",
    "print(result.markdown.raw_markdown) # Raw markdown from cleaned html\n",
    "print(result.markdown.fit_markdown) # Most relevant content in markdown\n",
    "\n",
    "# Check success status\n",
    "print(result.success)      # True if crawl succeeded\n",
    "print(result.status_code)  # HTTP status code (e.g., 200, 404)\n",
    "\n",
    "# Access extracted media and links\n",
    "print(result.media)        # Dictionary of found media (images, videos, audio)\n",
    "print(result.links)        # Dictionary of internal and external links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
