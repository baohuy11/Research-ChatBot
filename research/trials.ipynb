{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7eb63b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyenphungbaohuy/Desktop/python/Research-ChatBot/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2632f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f82b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyenphungbaohuy/Desktop/python/Research-ChatBot'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_PAPERS_URL = \"https://huggingface.co\"\n",
    "PDF_URL = \"https://arxiv.org/pdf/\"\n",
    "DATE = \"2025-04-10\"\n",
    "PAPER_DATE = f\"/papers/date/{DATE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7164fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0f90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str, date: str) -> str:\n",
    "    full_url = f\"{url}{date}\"\n",
    "    response = requests.get(full_url)\n",
    "    response.raise_for_status()\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a4e9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link(url: str) -> str:\n",
    "    links = {\n",
    "        \"arxiv_id\": \"\",\n",
    "        \"arxiv_page\": \"\",\n",
    "        \"arxiv_pdf\": \"\",\n",
    "        \"project_page\": \"\",\n",
    "        \"github_page\": \"\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        full_url = urljoin(url, href)\n",
    "        if re.search(r'arxiv\\.org/abs/\\d+\\.\\d+', href):\n",
    "            links[\"arxiv_page\"] = full_url\n",
    "            arxiv_id = href.split(\"/\")[-1]\n",
    "            links[\"arxiv_id\"] = arxiv_id\n",
    "            links[\"arxiv_pdf\"] = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        elif 'github.com' in href:\n",
    "            links[\"github_page\"] = full_url\n",
    "        elif 'Project page' in a.get_text(strip=True):\n",
    "            links[\"project_page\"] = full_url\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f858de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_papers(url: str, html: str) -> list[dict]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    papers = []\n",
    "    entries = soup.find_all(\"article\", class_=\"relative flex flex-col overflow-hidden rounded-xl border\")\n",
    "    for entry in entries:\n",
    "        info = entry.find(\"a\", class_=\"line-clamp-3 cursor-pointer text-balance\")\n",
    "        \n",
    "        paper_href = info.get(\"href\")\n",
    "        title = info.get_text(strip=True)\n",
    "        paper_url = urljoin(url, paper_href)\n",
    "        \n",
    "        additional_links = extract_link(paper_url)\n",
    "        \n",
    "        paper_info = {\n",
    "            \"title\": title,\n",
    "            \"huggingface_url\": paper_url,\n",
    "            **additional_links\n",
    "        }\n",
    "        \n",
    "        papers.append(paper_info)\n",
    "        \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "977c9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_today_papers(url: str, date: str) -> list[dict]:\n",
    "    html = fetch_page(url, date)\n",
    "    papers = parse_papers(HUGGINGFACE_PAPERS_URL, html)\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_papers = get_today_papers(HUGGINGFACE_PAPERS_URL, PAPER_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f395353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url: str, arxiv_id: str) -> str:\n",
    "    file_dir = \"data\"\n",
    "    os.makedirs(file_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(file_dir, f\"{arxiv_id}.pdf\")\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded and saved in: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ffcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved as data/OmniSVG: A Unified Scalable Vector Graphics Generation Model.pdf\n",
      "Downloaded and saved as data/Hogwild! Inference: Parallel LLM Generation via Concurrent Attention.pdf\n"
     ]
    }
   ],
   "source": [
    "for paper in today_papers[0:1]:\n",
    "    download_pdf(paper['arxiv_pdf'], paper['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8359587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data From the PDF File\n",
    "def load_pdf_file(data: str) -> str:\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a79fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text_split(extracted_data=extracted_data)\n",
    "print(f\"Lenght of chunck: {len(text_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e72ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings(model_name: str):\n",
    "    embeddings =  HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f63e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m qa_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepset/roberta-base-squad2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "embedding = download_hugging_face_embeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc656b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "HUGGINGFACE_API_KEY = os.enviro.get('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"researchbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca79742",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(name=index_name,\n",
    "                dimension=384,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\",\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db372515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunk,\n",
    "    index_name=index_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318581ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the emebeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a84af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cceb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(text: str) -> str:\n",
    "    max_chunk_length = 1024\n",
    "    \n",
    "    chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
    "    summary = \"\"\n",
    "    for chunk in chunks:\n",
    "        summarized_chunk = summarizer(chunk, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "        summary += summarized_chunk + \" \"\n",
    "    return summary.strip()\n",
    "\n",
    "def chatbot_interface(context: str):\n",
    "    print(\"Chatbot ready. Type your questions about the paper. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        question = input(\"Your question: \")\n",
    "        if question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Use the QA pipeline with the given context.\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        answer = result.get(\"answer\", \"I'm not sure about that.\")\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example paper input; you can later extend this\n",
    "    # to process multiple inputs.\n",
    "    paper_data = {\n",
    "        \"title\": (\"CrossWordBench: Evaluating the Reasoning Capabilities of LLMs and LVLMs \"\n",
    "                  \"with Controllable Puzzle Generation\"),\n",
    "        \"arxiv_pdf\": \"https://arxiv.org/pdf/2504.00043.pdf\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading PDF for paper: {paper_data['title']}\")\n",
    "        pdf_path = download_pdf(paper_data[\"arxiv_pdf\"])\n",
    "        \n",
    "        print(\"Extracting text from the PDF...\")\n",
    "        pdf_text = extract_pdf_text(pdf_path)\n",
    "        \n",
    "        print(\"Summarizing the paper...\")\n",
    "        summary = summarize_paper(pdf_text)\n",
    "        print(\"Paper Summary:\\n\", summary)\n",
    "        \n",
    "        # Start chatbot interface based on the summary.\n",
    "        chatbot_interface(summary)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"deepseek-r1\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a012b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "System Prompt for AI Research Assistant\n",
    "\n",
    "Role and Scope:\n",
    "You are a research assistant specialized in AI and large language models (LLMs). Your primary duties include discovering relevant papers on the Hugging Face platform, extracting PDF links, downloading the research documents, summarizing their contents, and indexing the results for efficient retrieval using a vectorstore database. Your output must be clear, detailed, and maintain academic rigor.\n",
    "\n",
    "Functional Responsibilities:\n",
    "\n",
    "1. Paper Discovery & Download:\n",
    "   - Crawling: Continually monitor the Hugging Face page (or pages) dedicated to AI and LLM research for new papers.\n",
    "   - Extraction: Identify and extract the PDF links from the page.\n",
    "   - Downloading: Automatically download each PDF to the local system for further processing.\n",
    "\n",
    "2. Content Summarization:\n",
    "   - Parsing: Process the downloaded PDF and extract key sections—such as the abstract, introduction, methodology, experiments, results, and conclusion.\n",
    "   - Summarizing: Generate a concise yet comprehensive summary that captures the paper’s main contributions, methods, findings, and any notable insights. Ensure the summary preserves the technical accuracy and context of the original document.\n",
    "   - Validation: Verify that every summary is faithful to the paper's content, and highlight any potential uncertainties or limitations in the document.\n",
    "\n",
    "3. Indexing with Vectorstore:\n",
    "   - Embedding: Convert the text summary or key points into vector embeddings that represent the paper’s content.\n",
    "   - Database Storage: Index these embeddings in the vectorstore to allow efficient and relevant retrieval during query time.\n",
    "   - Query Matching: When a user query is received, search the vectorstore to retrieve the most semantically related papers and provide contextualized, aggregated insights.\n",
    "\n",
    "4. User Query Response:\n",
    "   - Contextual Answers: When asked about specific topics or research areas, integrate information from the vectorstore and provide detailed answers.\n",
    "   - Direct References: When possible, refer to the paper sections (e.g., “based on the methodology described in section 3,”) to support your response.\n",
    "   - Clarity & Detail: Ensure all outputs are clear, logically structured, and include sufficient technical details, especially when addressing experimental setups or complex methodologies.\n",
    "\n",
    "Behavioral Guidelines:\n",
    "- Professional Tone: Maintain a scholarly and objective tone throughout all interactions.\n",
    "- Accuracy and Rigor: Base responses strictly on the data derived from the PDF documents. Avoid injecting personal opinions or unverified information.\n",
    "- Error Handling: If any issue arises (e.g., PDF parsing errors, incomplete downloads), log a clear error message that describes the problem and, if possible, suggest remedial steps.\n",
    "- Modularity: The agent should work in a modular fashion; for instance, if the PDF extraction fails, it must notify the user or log the error without disrupting other functionalities.\n",
    "\n",
    "Example Instruction for a Query:\n",
    "“When a user inquires about recent advances in transformer-based LLMs, search your indexed vectorstore for the most relevant papers, or the summarization of the papers. Summarize the key aspects of the top results—such as novel architectures, dataset insights, and experimental results—and provide a clear, consolidated answer that synthesizes these insights.”\n",
    "\n",
    "Usage Note:\n",
    "Embed this system prompt at the beginning of your agent’s session or configuration file to ensure that every module (crawling, summarizing, embedding, and querying) follows these guidelines. This will help the agent act in a coordinated manner, reliably reflecting the state-of-the-art research in AI LLMs from the Hugging Face page.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca170ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-based QA chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt=prompt)\n",
    "# Assume '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035d7450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c813f63adcac4cf39f452d4e373a5395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983c2b2210524a58925687598ef9a17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# This will download the model and create cache folders automatically\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
