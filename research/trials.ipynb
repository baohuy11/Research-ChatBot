{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7eb63b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyenphungbaohuy/Desktop/python/Research-ChatBot/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2632f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f82b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyenphungbaohuy/Desktop/python/Research-ChatBot'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7164fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b76b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a4e9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link(url: str) -> str:\n",
    "    links = {\n",
    "        \"arxiv_page\": \"\",\n",
    "        \"arxiv_pdf\": \"\",\n",
    "        \"project_page\": \"\",\n",
    "        \"github_page\": \"\",\n",
    "        \"summary\": \"\",\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        full_url = urljoin(url, href)\n",
    "        if re.search(r'arxiv\\.org/abs/\\d+\\.\\d+', href):\n",
    "            links[\"arxiv_page\"] = full_url\n",
    "            arxiv_id = href.split(\"/\")[-1]\n",
    "            links[\"arxiv_pdf\"] = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        elif 'github.com' in href:\n",
    "            links[\"github_page\"] = full_url\n",
    "        elif 'Project page' in a.get_text(strip=True):\n",
    "            links[\"project_page\"] = full_url\n",
    "        \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f858de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_papers(url: str, date: str) -> list[dict]:\n",
    "    full_url = f\"{url}{date}\"\n",
    "    response = requests.get(full_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    papers = []\n",
    "    entries = soup.find_all(\"article\", class_=\"relative flex flex-col overflow-hidden rounded-xl border\")\n",
    "    for entry in entries:\n",
    "        info = entry.find(\"a\", class_=\"line-clamp-3 cursor-pointer text-balance\")\n",
    "        \n",
    "        paper_href = info.get(\"href\")\n",
    "        title = info.get_text(strip=True)\n",
    "        paper_url = urljoin(url, paper_href)\n",
    "        \n",
    "        additional_links = extract_link(paper_url)\n",
    "        \n",
    "        paper_info = {\n",
    "            \"title\": title,\n",
    "            \"huggingface_url\": paper_url,\n",
    "            **additional_links\n",
    "        }\n",
    "        \n",
    "        papers.append(paper_info)\n",
    "        \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddfc31e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_locally(data, filename, directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    print(f'Data saved to {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0363c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pdf(url: str, summarizer) -> str:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "        \n",
    "    # Read the PDF content from bytes\n",
    "    pdf_file = BytesIO(response.content)\n",
    "    reader = PdfReader(pdf_file)\n",
    "        \n",
    "    # Extract text from each page of the PDF\n",
    "    extracted_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            extracted_text += page_text + \"\\n\"\n",
    "        \n",
    "    if not extracted_text.strip():\n",
    "        return \"No text could be extracted from the PDF.\"\n",
    "        \n",
    "    words = extracted_text.split()\n",
    "    truncated_text = \" \".join(words[:2000])\n",
    "        \n",
    "    # Generate a summary. You can tweak max_length and min_length to control output.\n",
    "    summary = summarizer(truncated_text, max_length=150, min_length=50, do_sample=False)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3a2c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    DATE = date.today()\n",
    "    # Paths\n",
    "    # BASE_DIR = Path(__file__).resolve().parent.parent.parent\n",
    "\n",
    "    # DATA_DIR = \"data/\"\n",
    "    DB_PATH = \"data/papers_db\"\n",
    "    # LOGS_DIR = DATA_DIR / \"logs\"\n",
    "    \n",
    "    # URLs\n",
    "    HUGGINGFACE_PAPERS_URL = \"https://huggingface.co\"\n",
    "    ARXIV_PDF_URL = \"https://arxiv.org/pdf/\"\n",
    "    PAPER_DATE = f\"/papers/date/{DATE}\"\n",
    "    \n",
    "    # Model config\n",
    "    SUMMARIZATION_MODEL = \"facebook/bart-large-cnn\"\n",
    "    EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    \n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ad894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer_model():\n",
    "    summarizer = pipeline(\"summarization\", model=settings.SUMMARIZATION_MODEL)\n",
    "    return summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "977c9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    today_papers = parse_papers(settings.HUGGINGFACE_PAPERS_URL, settings.PAPER_DATE)\n",
    "    summarizer = summarizer_model()\n",
    "    if not today_papers:\n",
    "        print(\"No papers found for today.\")\n",
    "        return\n",
    "        \n",
    "    print(len(today_papers))\n",
    "\n",
    "    for paper in today_papers:\n",
    "        if not paper['arxiv_pdf']:\n",
    "            print(f\"No PDF link found for {paper['title']}\")\n",
    "            continue\n",
    "\n",
    "        paper['summary'] = summarize_pdf(paper['arxiv_pdf'], summarizer)\n",
    "            \n",
    "    filename = f\"papers_{settings.DATE}.json\"\n",
    "    save_data_locally(today_papers, filename, settings.DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "146ff48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Data saved to data/papers_db/papers_2025-04-14.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f395353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url: str, arxiv_id: str) -> str:\n",
    "    file_dir = \"data/\"\n",
    "    os.makedirs(file_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(file_dir, f\"{arxiv_id}.pdf\")\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded and saved in: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ada4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "HUGGINGFACE_API_KEY = os.enviro.get('HUGGINGFACE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8359587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data From the PDF File\n",
    "def load_pdf_file(data: str) -> str:\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a79fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = text_split(extracted_data=extracted_data)\n",
    "print(f\"Lenght of chunck: {len(text_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e72ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings(model_name: str):\n",
    "    embeddings =  HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f63e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m qa_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepset/roberta-base-squad2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "embedding = download_hugging_face_embeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"researchbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca79742",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.create_index(name=index_name,\n",
    "                dimension=384,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\",\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db372515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunk,\n",
    "    index_name=index_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318581ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the emebeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a84af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc56a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"deepseek-r1\", temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a012b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "System Prompt for AI Research Assistant\n",
    "\n",
    "Role and Scope:\n",
    "You are a research assistant specialized in AI and large language models (LLMs). Your primary duties include discovering relevant papers on the Hugging Face platform, extracting PDF links, downloading the research documents, summarizing their contents, and indexing the results for efficient retrieval using a vectorstore database. Your output must be clear, detailed, and maintain academic rigor.\n",
    "\n",
    "Functional Responsibilities:\n",
    "\n",
    "1. Paper Discovery & Download:\n",
    "   - Crawling: Continually monitor the Hugging Face page (or pages) dedicated to AI and LLM research for new papers.\n",
    "   - Extraction: Identify and extract the PDF links from the page.\n",
    "   - Downloading: Automatically download each PDF to the local system for further processing.\n",
    "\n",
    "2. Content Summarization:\n",
    "   - Parsing: Process the downloaded PDF and extract key sections—such as the abstract, introduction, methodology, experiments, results, and conclusion.\n",
    "   - Summarizing: Generate a concise yet comprehensive summary that captures the paper’s main contributions, methods, findings, and any notable insights. Ensure the summary preserves the technical accuracy and context of the original document.\n",
    "   - Validation: Verify that every summary is faithful to the paper's content, and highlight any potential uncertainties or limitations in the document.\n",
    "\n",
    "3. Indexing with Vectorstore:\n",
    "   - Embedding: Convert the text summary or key points into vector embeddings that represent the paper’s content.\n",
    "   - Database Storage: Index these embeddings in the vectorstore to allow efficient and relevant retrieval during query time.\n",
    "   - Query Matching: When a user query is received, search the vectorstore to retrieve the most semantically related papers and provide contextualized, aggregated insights.\n",
    "\n",
    "4. User Query Response:\n",
    "   - Contextual Answers: When asked about specific topics or research areas, integrate information from the vectorstore and provide detailed answers.\n",
    "   - Direct References: When possible, refer to the paper sections (e.g., “based on the methodology described in section 3,”) to support your response.\n",
    "   - Clarity & Detail: Ensure all outputs are clear, logically structured, and include sufficient technical details, especially when addressing experimental setups or complex methodologies.\n",
    "\n",
    "Behavioral Guidelines:\n",
    "- Professional Tone: Maintain a scholarly and objective tone throughout all interactions.\n",
    "- Accuracy and Rigor: Base responses strictly on the data derived from the PDF documents. Avoid injecting personal opinions or unverified information.\n",
    "- Error Handling: If any issue arises (e.g., PDF parsing errors, incomplete downloads), log a clear error message that describes the problem and, if possible, suggest remedial steps.\n",
    "- Modularity: The agent should work in a modular fashion; for instance, if the PDF extraction fails, it must notify the user or log the error without disrupting other functionalities.\n",
    "\n",
    "Example Instruction for a Query:\n",
    "“When a user inquires about recent advances in transformer-based LLMs, search your indexed vectorstore for the most relevant papers, or the summarization of the papers. Summarize the key aspects of the top results—such as novel architectures, dataset insights, and experimental results—and provide a clear, consolidated answer that synthesizes these insights.”\n",
    "\n",
    "Usage Note:\n",
    "Embed this system prompt at the beginning of your agent’s session or configuration file to ensure that every module (crawling, summarizing, embedding, and querying) follows these guidelines. This will help the agent act in a coordinated manner, reliably reflecting the state-of-the-art research in AI LLMs from the Hugging Face page.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca170ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document-based QA chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt=prompt)\n",
    "# Assume '"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
